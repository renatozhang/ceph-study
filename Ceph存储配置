# Ceph存储配置

## 创建Ceph块设备

**Rados块设备（RBD）： 前身是ceph块设备，为Ceph客户端提供基于块的持久化存储，通常作为一个额外的磁盘使用。客户可以灵活地使用这个磁盘，可以作为裸设备使用，也可以格式化成文件系统，然后挂载它。RBD利用librbd库并将块数据连续地存储在跨越多个OSD的条带装存储空间中。RBD由Ceph的Rados层提供支持，因此每个块设备都分布多个Ceph节点上，从文提供了高性能和出色的可靠性。RBD具有丰富的企业特性，如自动精简配置（thin provisioning）、动态调整容量、快照、写时复制以及缓存等。RBD协议作为主线内核驱动程序有Linux完全支持；它还支持多种虚拟化平台，如KVM、QEMU和libvirt，使得虚拟机可以利用Ceph块设备。所有这些特点使RBD成为云平台（例如Openstack和Cloudstack）的一个理想选择。**


## 创建Ceph文件系统

**要创建Ceph的块设备，需要先登录Ceph集群的任意一个monitor节点，或者登录具有ceph集群管理员权限的一台管理主机。也可以在任意一个Ceph客户端节点上创建Ceph RBD。出于安全考虑，不应该吧Ceph管理秘钥保存在Ceph节点和管理主机之外的机器上**

### 创建一个名为ceph-client1-rbd1,大小为10240MB的RBD设备
#### 首先创建一个RBD存储池
	[root@ceph-node1 ~]# ceph osd pool  create rbd 128 128  
#### 创建块设备
	[root@ceph-node1 ~]# rbd create ceph-client1-rbd1 --size 10240
#### 列出RBD镜像的详细信息
	[root@ceph-node1 ~]# rbd ls
	ceph-client1-rbd1
#### 检查一个rbd镜像的详细信息，
	[root@ceph-node1 ~]# rbd --image ceph-client1-rbd1 info
	rbd image 'ceph-client1-rbd1':
	        size 10 GiB in 2560 objects
	        order 22 (4 MiB objects)
	        snapshot_count: 0
	        id: 176344ce5ef1
	        block_name_prefix: rbd_data.176344ce5ef1
	        format: 2
	        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	        op_features: 
	        flags: 
	        create_timestamp: Sat Oct 12 08:40:11 2019
	        access_timestamp: Sat Oct 12 08:40:11 2019
	        modify_timestamp: Sat Oct 12 08:40:11 2019
#### 默认情况下，RBD镜像会存储在ceph集群的rbd存储池中，使用-p参数指定其它池。
	[root@ceph-node1 ~]# rbd --image ceph-client1-rbd1 info -p rbd

#### 移除rbd
	[root@ceph-node1 ~]# rbd remove ceph-client-rbd1

### 创建第一个ceph客户端
	创建一台ceph-client的虚拟机
### 映射RADOS块设备
#### 查看内核版本是否支持ceph
**Linux 内核从2.6.32版本开始支持ceph。对于客户端机器来说，要支持本地化方式访问ceph块设备和文件系统，建议使用2.6.34及以上版本的Linux内核。**

	[root@ceph-client1 ~]# uname -r
	3.10.0-1062.el7.x86_64
	[root@ceph-client1 ~]# modprobe rbd
	[root@ceph-client1 ~]# 
#### 为ceph-client1安装ceph二进制包
[root@ceph-client1 ~]# yum instlall -y ceph
#### 映射ceph-client1-rbd1的RBD的镜像到ceph-client1节点
	[root@ceph-client1 ~]# rbd map --image ceph-client1-rbd1
	rbd: sysfs write failed
	RBD image feature set mismatch. You can disable features unsupported by the kernel with "rbd feature disable ceph-client1-rbd1 object-map fast-diff deep-flatten".
	In some cases useful info is found in syslog - try "dmesg | tail".
	rbd: map failed: (6) No such device or address
	# ceph新版中在map image时，给image默认加上了许多feature，通过rbd info可以查看到：
		[root@ceph-node1 ~]# rbd --image ceph-client1-rbd1 info
	rbd image 'ceph-client1-rbd1':
	        size 10 GiB in 2560 objects
	        order 22 (4 MiB objects)
	        snapshot_count: 0
	        id: 176344ce5ef1
	        block_name_prefix: rbd_data.176344ce5ef1
	        format: 2
	        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	        op_features: 
	        flags: 
	layering: 支持分层
	striping: 支持条带化 v2
	exclusive-lock: 支持独占锁
	object-map: 支持对象映射（依赖 exclusive-lock ）
	fast-diff: 快速计算差异（依赖 object-map ）
	deep-flatten: 支持快照扁平化操作
	journaling: 支持记录 IO 操作（依赖独占锁）
#### CentOS的3.10内核仅支持其中的layering feature，其他feature概不支持。我们需要手动disable这些features：
	[root@ceph-client1 ~]# rbd feature disable ceph-client1-rbd1 exclusive-lock, object-map, fast-diff, deep-flatten 
	# 重新映射rbd-client1-rbd1
	[root@ceph-client1 ~]# rbd map ceph-client1-rbd1
	/dev/rbd0
	# 查看本次映射对应的操作系统设备名
	[root@ceph-client1 ~]# rbd showmapped
	id pool namespace image             snap device    
	0  rbd            ceph-client1-rbd1 -    /dev/rbd0 
#### 升级内核获得新特性支持
**Ceph 10.x (Jewel),应该至少使用4.x内核。
##### 启用 ELRepo 仓库

	#导入ELRepo仓库的公共密钥
	rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
	#安装ELRepo仓库的yum源
	rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
##### 查看可用的系统内核包
	[root@ceph-client1 ~]#  yum --disablerepo="*" --enablerepo="elrepo-kernel" list available
	Loaded plugins: fastestmirror
	Loading mirror speeds from cached hostfile
	 * elrepo-kernel: mirrors.neusoft.edu.cn
	Available Packages
	elrepo-release.noarch                 7.0-4.el7.elrepo          elrepo-kernel
	kernel-lt.x86_64                      4.4.196-1.el7.elrepo      elrepo-kernel
	kernel-lt-devel.x86_64                4.4.196-1.el7.elrepo      elrepo-kernel
	kernel-lt-doc.noarch                  4.4.196-1.el7.elrepo      elrepo-kernel
	kernel-lt-headers.x86_64              4.4.196-1.el7.elrepo      elrepo-kernel
	kernel-lt-tools.x86_64                4.4.196-1.el7.elrepo      elrepo-kernel
	kernel-lt-tools-libs.x86_64           4.4.196-1.el7.elrepo      elrepo-kernel
	kernel-lt-tools-libs-devel.x86_64     4.4.196-1.el7.elrepo      elrepo-kernel
	kernel-ml-devel.x86_64                5.3.5-1.el7.elrepo        elrepo-kernel
	kernel-ml-doc.noarch                  5.3.5-1.el7.elrepo        elrepo-kernel
	kernel-ml-headers.x86_64              5.3.5-1.el7.elrepo        elrepo-kernel
	kernel-ml-tools.x86_64                5.3.5-1.el7.elrepo        elrepo-kernel
	kernel-ml-tools-libs.x86_64           5.3.5-1.el7.elrepo        elrepo-kernel
	kernel-ml-tools-libs-devel.x86_64     5.3.5-1.el7.elrepo        elrepo-kernel
	perf.x86_64                           5.3.5-1.el7.elrepo        elrepo-kernel
	python-perf.x86_64                    5.3.5-1.el7.elrepo        elrepo-kernel
	[root@ceph-client1 ~]# yum --enablerepo=elrepo-kernel install kernel-ml
        # 然后优雅的重启机器
	[root@ceph-client1 ~]#  awk -F\' '$1=="menuentry " {print i++ " : " $2}' /etc/grub2.cfg
	0 : CentOS Linux (5.3.5-1.el7.elrepo.x86_64) 7 (Core)
	1 : CentOS Linux (3.10.0-1062.el7.x86_64) 7 (Core)
	2 : CentOS Linux (0-rescue-3fd40120db4e49d6a54a9ee4c23d1722) 7 (Core)
##### 设置新得内核为grub2的默认版本
1. 通过grub2-set-default 0 命令设置
	 [root@ceph-client1 ~]# grub2-set-default 0
2. 编辑 /etc/default/grub 文件
	[root@ceph-client1 ~]# vim /etc/default/grub 
	GRUB_TIMEOUT=5
	GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
	GRUB_DEFAULT=0
	GRUB_DISABLE_SUBMENU=true
	GRUB_TERMINAL_OUTPUT="console"
	GRUB_CMDLINE_LINUX="crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet"
	GRUB_DISABLE_RECOVERY="true"

#### RBD映射到OS，创建文件系统
	[root@ceph-client1 ~]# fdisk -l /dev/rbd0 
	Disk /dev/rbd0: 10.7 GB, 10737418240 bytes, 20971520 sectors
	Units = sectors of 1 * 512 = 512 bytes
	Sector size (logical/physical): 512 bytes / 512 bytes
	I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes
#### 作为额外的磁盘或者设备使用
	[root@ceph-client1 ~]# mkfs.xfs /dev/rbd0
	meta-data=/dev/rbd0              isize=512    agcount=16, agsize=163840 blks
	         =                       sectsz=512   attr=2, projid32bit=1
	         =                       crc=1        finobt=0, sparse=0
	data     =                       bsize=4096   blocks=2621440, imaxpct=25
	         =                       sunit=1024   swidth=1024 blks
	naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
	log      =internal log           bsize=4096   blocks=2560, version=2
	         =                       sectsz=512   sunit=8 blks, lazy-count=1
	realtime =none                   extsz=4096   blocks=0, rtextents=0
	[root@ceph-client1 ~]# mkdir /mnt/ceph-vol1
	[root@ceph-client1 ~]# mount /dev/r
	random  raw/    rbd/    rbd0    rtc     rtc0    
	[root@ceph-client1 ~]# mount /dev/r
	random  raw/    rbd/    rbd0    rtc     rtc0    
	[root@ceph-client1 ~]# mount /dev/rbd0 /mnt/ceph-vol1/
	[root@ceph-client1 ~]# df -h
	Filesystem               Size  Used Avail Use% Mounted on
	devtmpfs                 475M     0  475M   0% /dev
	tmpfs                    487M     0  487M   0% /dev/shm
	tmpfs                    487M  7.7M  479M   2% /run
	tmpfs                    487M     0  487M   0% /sys/fs/cgroup
	/dev/mapper/centos-root   17G  1.7G   16G  10% /
	/dev/sda1               1014M  136M  879M  14% /boot
	tmpfs                     98M     0   98M   0% /run/user/0
	/dev/rbd0                 10G   33M   10G   1% /mnt/ceph-vol1
	
	[root@ceph-client1 ~]# dd if=/dev/zero of=/mnt/ceph-vol1/file1 count=1000 bs=1M
	1000+0 records in
	1000+0 records out
	1048576000 bytes (1.0 GB) copied, 14.043 s, 74.7 MB/s
	[root@ceph-client1 ~]# ls -la /mnt/ceph-vol1/
	total 1024000
	drwxr-xr-x  2 root root         19 Oct 12 17:56 .
	drwxr-xr-x. 3 root root         23 Oct 12 17:55 ..
	-rw-r--r--  1 root root 1048576000 Oct 12 17:57 file1

#### 首先创建一个RBD存储池
	[root@ceph-node1 ~]# ceph osd pool  create testrbd 256 256  
#### 创建块设备
	[root@ceph-node1 ~]# rbd pool init testrbd
	[root@ceph-node1 ~]# rbd create --size 10G testrbd/ceph-client1-rbd1
#### 列出RBD镜像的详细信息
	[root@ceph-node1 ~]# rbd ls testrbd 
	ceph-client1-rbd1
#### 检查一个rbd镜像的详细信息，
	[root@ceph-node1 ~]# rbd --image testrbd/ceph-client1-rbd1 info    
rbd image 'ceph-client1-rbd1':
        size 10 GiB in 2560 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 197404345a408
        block_name_prefix: rbd_data.197404345a408
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features: 
        flags: 
        create_timestamp: Sat Oct 12 11:23:35 2019
        access_timestamp: Sat Oct 12 11:23:35 2019
        modify_timestamp: Sat Oct 12 11:23:35 2019
#### 默认情况下，RBD镜像会存储在ceph集群的rbd存储池中，使用-p参数指定其它池。
	[root@ceph-node1 ~]# rbd --image  ceph-client1-rbd1 info -p testrbd

#### RBD映射到OS，创建文件系统
	[root@ceph-client1 ~]# rbd map  testrbd/ceph-client1-rbd1
	/dev/rbd0
	[root@ceph-client1 ~]# rbd showmapped                    
	id pool    namespace image             snap device    
	0  testrbd           ceph-client1-rbd1 -    /dev/rbd0 

#### 作为额外的磁盘或者设备使用
	[root@ceph-client1 ~]# fdisk -l
	
	Disk /dev/sda: 21.5 GB, 21474836480 bytes, 41943040 sectors
	Units = sectors of 1 * 512 = 512 bytes
	Sector size (logical/physical): 512 bytes / 512 bytes
	I/O size (minimum/optimal): 512 bytes / 512 bytes
	Disk label type: dos
	Disk identifier: 0x000c3110
	
	   Device Boot      Start         End      Blocks   Id  System
	/dev/sda1   *        2048     2099199     1048576   83  Linux
	/dev/sda2         2099200    41943039    19921920   8e  Linux LVM
	
	Disk /dev/mapper/centos-root: 18.2 GB, 18249416704 bytes, 35643392 sectors
	Units = sectors of 1 * 512 = 512 bytes
	Sector size (logical/physical): 512 bytes / 512 bytes
	I/O size (minimum/optimal): 512 bytes / 512 bytes
	
	
	Disk /dev/mapper/centos-swap: 2147 MB, 2147483648 bytes, 4194304 sectors
	Units = sectors of 1 * 512 = 512 bytes
	Sector size (logical/physical): 512 bytes / 512 bytes
	I/O size (minimum/optimal): 512 bytes / 512 bytes
	
	
	Disk /dev/rbd0: 10.7 GB, 10737418240 bytes, 20971520 sectors
	Units = sectors of 1 * 512 = 512 bytes
	Sector size (logical/physical): 512 bytes / 512 bytes
	I/O size (minimum/optimal): 65536 bytes / 65536 bytes
	
	[root@ceph-client1 ~]# mkfs.xfs /dev/rbd0
	meta-data=/dev/rbd0              isize=512    agcount=16, agsize=163840 blks
	         =                       sectsz=512   attr=2, projid32bit=1
	         =                       crc=1        finobt=0, sparse=0
	data     =                       bsize=4096   blocks=2621440, imaxpct=25
	         =                       sunit=16     swidth=16 blks
	naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
	log      =internal log           bsize=4096   blocks=2560, version=2
	         =                       sectsz=512   sunit=16 blks, lazy-count=1
	realtime =none                   extsz=4096   blocks=0, rtextents=0

	raw/rbd/ 
	[root@ceph-client1 ~]# mkdir /mnt/ceph-vol1
	[root@ceph-client1 ~]# mount /dev/rbd0 /mnt/ceph-vol1
	[root@ceph-client1 ~]# df -h
	Filesystem               Size  Used Avail Use% Mounted on
	devtmpfs                 465M     0  465M   0% /dev
	tmpfs                    478M     0  478M   0% /dev/shm
	tmpfs                    478M  6.8M  472M   2% /run
	tmpfs                    478M     0  478M   0% /sys/fs/cgroup
	/dev/mapper/centos-root   17G  1.8G   16G  11% /
	/dev/sda1               1014M  169M  846M  17% /boot
	tmpfs                     96M     0   96M   0% /run/user/0
	/dev/rbd0                 10G   33M   10G   1% /mnt/ceph-vol1

#### 向CephRBD上存放数据
	[root@ceph-client1 ~]# dd if=/dev/zero of=/mnt/ceph-vol1/file1 count=1000 bs=1M
	[root@ceph-client1 ~]# ls -la /mnt/ceph-vol1/
	total 1024000
	drwxr-xr-x  2 root root         19 Oct 12 11:43 .
	drwxr-xr-x. 3 root root         23 Oct 12 11:42 ..
	-rw-r--r--  1 root root 1048576000 Oct 12 11:43 file1

### 调整Ceph RBD的大小
Ceph支持自动精简配置的块设备，只有当把数据存储到这个块设备时，才会真正地使用物理存储空间。Ceph RADOS块设备非常灵活，可以自由增加和减少RBD的容量。当然，这需要底层的文件系统也支持调整容量。高级稳健系统（例如XFS，Btrfs,EXT和ZFS）都支持在指定条件下调整文件系统容量。
#### 使用rbd  resize 命令的--size增加和减少Ceph镜像的容量
	# 将RBD容量扩展到20G
	[root@ceph-client1 ~]# rbd resize --size 20G testrbd/ceph-client1-rbd1 
	# 将RDB容量缩减到10G 使用--allow-shrink
	[root@ceph-client1 ~]# rbd resize --size 10G testrbd/ceph-client1-rbd1 --allow-shrink
        # xfs文件系统不支持缩容 如果缩容可选择先unount 配合xfs_dump和xfs_restore进行缩容
#### 容量调整完成后，使用xfs_grown检查容量是否已被内核所接收
	[root@ceph-client1 ~]# xfs_growfs -d /mnt/ceph-vol1/
	meta-data=/dev/rbd0              isize=512    agcount=16, agsize=163840 blks
	         =                       sectsz=512   attr=2, projid32bit=1
	         =                       crc=1        finobt=0 spinodes=0
	data     =                       bsize=4096   blocks=2621440, imaxpct=25
	         =                       sunit=16     swidth=16 blks
	naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
	log      =internal               bsize=4096   blocks=2560, version=2
	         =                       sectsz=512   sunit=16 blks, lazy-count=1
	realtime =none                   extsz=4096   blocks=0, rtextents=0
	data blocks changed from 2621440 to 5242880
	
	[root@ceph-client1 ~]# df -h
	Filesystem               Size  Used Avail Use% Mounted on
	devtmpfs                 465M     0  465M   0% /dev
	tmpfs                    478M     0  478M   0% /dev/shm
	tmpfs                    478M  6.8M  472M   2% /run
	tmpfs                    478M     0  478M   0% /sys/fs/cgroup
	/dev/mapper/centos-root   17G  1.8G   16G  11% /
	/dev/sda1               1014M  169M  846M  17% /boot
	tmpfs                     96M     0   96M   0% /run/user/0
	/dev/rbd0                 20G  1.1G   19G   6% /mnt/ceph-vol1
在客户端机器上，还需要扩展文件系统容量，以便能够使用新增的存储空间。从客户端角度来说，容量调整是OS文件停的特性。在调整热和分区容量之前，应该查阅相关文件系统文档。XFS文件系统支持在线调整容量。

#### Ceph RBD快照

Ceph完全支持快照，他是一个基于时间点的、制度RBD=镜像文件。可以通过创建快照恢复期原始数据，保存Ceph RBD镜像的状态。

##### 为了测试Ceph RBD的快照功能，在RBD上创建一个文件
	[root@ceph-client1 ~]# echo "Hello Ceph This is snapshot test by zhang" > /mnt/ceph-vol1/snaptest_file
	[root@ceph-client1 ~]# ll /mnt/ceph-vol1/
	total 1024004
	-rw-r--r-- 1 root root 1048576000 Oct 12 14:29 file
	-rw-r--r-- 1 root root         42 Oct 12 19:44 snaptest_file
##### 创建快照
	[root@ceph-client1 ~]# rbd snap create testrbd/ceph-client1-rbd1@snap1
	[root@ceph-client1 ~]# rbd snap ls testrbd/ceph-client1-rbd1 
	SNAPID NAME      SIZE 
	    12 snap1 20480 MB 
##### 从文件系统中删除文件
	[root@ceph-client1 ~]# rm -rf /mnt/ceph-vol1/snaptest_file 
	[root@ceph-client1 ~]# ll /mnt/ceph-vol1/
	total 1024000
	-rw-r--r-- 1 root root 1048576000 Oct 12 14:29 file
##### 执行回滚操作（回滚操作使用快照版本覆盖RBD镜像当前版本以及数据，应谨慎操作）
	[root@ceph-client1 ~]# rbd snap rollback testrbd/ceph-client1-rbd1@snap1 
	Rolling back to snapshot: 0% complete...failed.
	rbd: rollback failed: (30) Read-only file system
	# 出现次错误是ceph新特性rbd独占锁exclusive-lock设置开启的原因
	# 客户端umount挂在，取消map rdb映射
	[root@ceph-client1 ~]# umount /mnt/ceph-vol1/
	[root@ceph-client1 ~]# rbd unmap testrbd/ceph-client1-rbd1
	# 通过快照进行回滚，注意快照回滚，之前的数据会被覆盖
	[root@ceph-client1 ~]# rbd snap rollback testrbd/ceph-client1-rbd1@snap1
	Rolling back to snapshot: 100% complete...done.
	# 重新映射rbd并挂载文件系统，原来的文件恢复
	[root@ceph-client1 ~]# rbd map testrbd/ceph-client1-rbd1
	/dev/rbd0
	[root@ceph-client1 ~]# mount /dev/rbd0 /mnt/ceph-vol1
	[root@ceph-client1 ~]# df -h
	Filesystem               Size  Used Avail Use% Mounted on
	devtmpfs                 465M     0  465M   0% /dev
	tmpfs                    478M     0  478M   0% /dev/shm
	tmpfs                    478M  6.8M  472M   2% /run
	tmpfs                    478M     0  478M   0% /sys/fs/cgroup
	/dev/mapper/centos-root   17G  1.6G   16G  10% /
	/dev/sda1               1014M  169M  846M  17% /boot
	tmpfs                     96M     0   96M   0% /run/user/0
	/dev/rbd0                 20G  1.1G   19G   6% /mnt/ceph-vol1
	[root@ceph-client1 ~]# ll /mnt/ceph-vol1/
	total 1024004
	-rw-r--r-- 1 root root 1048576000 Oct 12 14:29 file
	-rw-r--r-- 1 root root         42 Oct 12 19:44 snaptest_file
	
#### 删除快照
##### 删除不需要的某一个快照
	[root@ceph-client1 ~]# rbd snap rm testrbd/ceph-client1-rbd1@snap1
	2019-10-12 20:03:06.881878 7fdbbb7fe700 -1 librbd::object_map::InvalidateRequest: 0x7fdbb40057e0 should_complete: r=0
##### 如果RBD有多各快照，并希望一条命令删除所有快照
	[root@ceph-client1 ~]# rbd snap purge testrbd/ceph-client1-rbd1
	
##### 删除RBD镜像
	[root@ceph-node1 ~]# rbd rm ceph-client-rbd1 -p testrbd

### 复制 Ceph RBD
Ceph支持创建块设备快照的许多写时复制（COW）克隆的功能。快照分层使Ceph块设备客户端可以非常快速地创建镜像。例如，您可以创建一个写入了Linux VM的块设备映像。然后，对映像进行快照，保护快照，并根据需要创建尽可能多的写时复制克隆。快照是只读的，因此克隆快照可以简化语义，从而可以快速创建克隆。
每个克隆的映像（子级）都存储对其父映像的引用，这使克隆的映像可以打开父快照并读取它。

快照的COW克隆的行为与任何其他Ceph块设备映像完全相同。您可以读取，写入，克隆和调整克隆镜像的大小。克隆镜像没有特殊限制。但是，快照的写时复制克隆引用快照，因此在克隆快照之前必须保护快照。下图描述了该过程。

注意 Ceph仅支持克隆格式2的镜像（即使用创建 ）。从内核3.10开始，内核客户端支持克隆的映像。rbd create --image-format 2 
#### 创建一个format-2 类型的RBD镜像
	[root@ceph-node1 ~]# rbd create testrbd/ceph-client1-rbd2 --size 10G
	# 如果ceph版本低默认为format-1 可以使用--format 2指定镜像格式
	[root@ceph-node1 ~]# rbd create testrbd/ceph-client1-rbd2 --size 10G --format 2

## 分层入门
Ceph块设备分层是一个简单的过程。您必须有有一个镜像。您必须创建镜像的快照。您必须保护快照。完成这些步骤后，即可开始克隆快照。
### 创建改RBD的快照
	[root@ceph-node1 ~]# rbd snap create testrbd/ceph-client1-rbd2@snapshot_for_clone

克隆的映像具有对父快照的引用，并且包括池ID，映像ID和快照ID。包含池ID意味着您可以将快照从一个池克隆到另一个池中的映像。

映像模板：块设备分层的一个常见用例是创建一个主映像和一个快照，以用作克隆的模板。例如，用户可以为Linux发行版（例如Ubuntu 12.04）创建映像，并为其创建快照。周期性地，用户可更新镜像，并创建新的快照（例如，，， 随后）。随着映像的成熟，用户可以克隆任何快照。sudo apt-get updatesudo apt-get upgradesudo apt-get dist-upgraderbd snap create

扩展模板：一个更高级的用例包括扩展一个模板镜像，该模板镜像提供的信息比基本镜像更多。例如，用户可以克隆映像（例如，VM模板）并安装其他软件（例如，数据库，内容管理系统，分析系统等），然后快照扩展的映像，该映像本身可以被更新就像基本图片一样

模板池：使用块设备分层的一种方法是创建一个池，其中包含充当模板的主映像以及这些模板的快照。然后，您可以将只读特权扩展给用户，以便他们可以克隆快照而无法在池中写入或执行。

映像迁移/恢复：使用块设备分层的一种方法是将数据从一个池迁移或恢复到另一个池。

#### 保护快照
克隆访问父快照。如果用户无意中删除了父快照，则所有克隆都将中断，所有连接它的COW副本都将被销毁。为防止数据丢失，必须在克隆快照之前保护快照。
#### 创建快照保护
	[root@ceph-node1 ~]# rbd snap protect testrbd/ceph-client1-rbd2@snapshot_for_clone

#### 克隆快照
要克隆快照，请指定您需要指定父池，映像和快照。以及子池和映像名称。在克隆快照之前，必须保护快照。
#### 根据快照克隆一个RBD
	[root@ceph-node1 ~]# rbd clone testrbd/ceph-client1-rbd2@snapshot_for_clone testrbd/ceph-client1-rbd3
创建快照速度非常快，创建完成后，查看新镜像的信息，会看到镜像的父池、镜像，以及快照的信息显示出来
	[root@ceph-node1 ~]# rbd --image testrbd/ceph-client1-rbd3 info
	rbd image 'ceph-client1-rbd3':
        size 10 GiB in 2560 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 19da08680ff92
        block_name_prefix: rbd_data.19da08680ff92
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features: 
        flags: 
        create_timestamp: Sun Oct 13 10:23:04 2019
        access_timestamp: Sun Oct 13 10:23:04 2019
        modify_timestamp: Sun Oct 13 10:23:04 2019
        parent: testrbd/ceph-client1-rbd2@snapshot_for_clone
        overlap: 10 GiB

#### 列出快照子级
要列出快照的子代，请执行以下操作：
	[root@ceph-node1 ~]# rbd children testrbd/ceph-client1-rbd2@snapshot_for_clone
	testrbd/ceph-client1-rbd3
#### 克隆的镜像的扁平化
克隆的映像保留对父快照的引用。当您从子克隆删除对父快照的引用时，可以通过将信息从快照复制到克隆来有效地“平整”映像。扁平克隆所需的时间随快照的大小而增加。要删除快照，必须先对镜像扁平化。
#### 扁平化操作后会看到父镜像@快照的明细消失了
	[root@ceph-node1 ~]# rbd flatten testrbd/ceph-client1-rbd3
	Image flatten: 100% complete...done.

	[root@ceph-node1 ~]# rbd --image testrbd/ceph-client1-rbd3 info
	rbd image 'ceph-client1-rbd3':
	        size 10 GiB in 2560 objects
	        order 22 (4 MiB objects)
	        snapshot_count: 0
	        id: 19da08680ff92
	        block_name_prefix: rbd_data.19da08680ff92
	        format: 2
	        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	        op_features: 
	        flags: 
	        create_timestamp: Sun Oct 13 10:23:04 2019
	        access_timestamp: Sun Oct 13 10:23:04 2019
	        modify_timestamp: Sun Oct 13 10:23:04 2019


#### 取消快照保护
在删除快照之前，必须先取消保护快照。此外，您可能不删除有来自克隆引用快照。必须先展平快照的每个克隆，然后才能删除快照。
	[root@ceph-node1 ~]# rbd snap  unprotect testrbd/ceph-client1-rbd2@snapshot_for_clone
#### 删除快照
	[root@ceph-node1 ~]# rbd snap rm testrbd/ceph-client1-rbd2@snapshot_for_clone
	Removing snap: 100% complete...done.

## Ceph 文件系统
eph文件系统或CephFS是在Ceph的分布式对象存储RADOS之上构建的POSIX兼容文件系统。并使用Ceph RADOS 存储数据。要实现Ceph，需要一个正常运行的Ceph存储集群，并且至少包含一个Ceph元数据服务器（Ceph Metadata Server,MDS）

两种Ceph文件系统使用方式：

1. 使用本地内核驱动程序挂在CephFS

### 检查客户端的Linux内核版本
	[root@ceph-client1 ~]# uname -r
	5.3.5-1.el7.elrepo.x86_64
### 创建挂在点目录
	[root@ceph-client1 ~]# mkdir /mnt/kernel_cephfs
### 记录管理员秘钥
	[root@ceph-client1 ~]# cat /etc/ceph/ceph.client.admin.keyring 
	[client.admin]
	        key = AQDKyZ9dQzpBEBAArba473MpELOhcGFlHomt1g==
	        auid = 0
	        caps mds = "allow *"
	        caps mgr = "allow *"
	        caps mon = "allow *"
	        caps osd = "allow *"
### 创建数据池和元数据池

使用默认设置创建两个池以供文件系统使用

	[root@ceph-node1 ~]# ceph osd pool create cephfs_data 64 64
	pool 'cephfs_data' created
	[root@ceph-node1 ~]# ceph osd pool create cephfs_metadata 64 64
	pool 'cephfs_metadata' created

通常，元数据池最多具有几GB的数据。因此，通常建议使用较少的PG。实际上，大型群集通常使用64或128。

### 创建文件系统
创建池后，可以使用一下命令启用文件系统 fs new

	[root@ceph-node1 ~]# ceph fs new cephfs cephfs_metadata  cephfs_data
	new fs with metadata pool 4 and data pool 3
	[root@ceph-node1 ~]# ceph fs ls
	name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data]

创建文件系统后，您的MDS将能够进入活动状态。例如，在单个MDS系统中：

	[root@ceph-node1 ~]# ceph mds stat
	cephfs:1 {0=ceph-node2=up:active}
### 使用原生Linux mount命令挂在CephFS
	[root@ceph-client1 ~]# mount -t ceph 192.168.1.102:6789:/ /mnt/kernel_cephfs -o name=admin,secret=AQDKyZ9dQzpBEBAArba473MpELOhcGFlHomt1g==
	[root@ceph-client1 ~]# df -h
	Filesystem               Size  Used Avail Use% Mounted on
	devtmpfs                 465M     0  465M   0% /dev
	tmpfs                    478M     0  478M   0% /dev/shm
	tmpfs                    478M  6.7M  472M   2% /run
	tmpfs                    478M     0  478M   0% /sys/fs/cgroup
	/dev/mapper/centos-root   17G  1.6G   16G  10% /
	/dev/sda1               1014M  169M  846M  17% /boot
	tmpfs                     96M     0   96M   0% /run/user/0
	/dev/rbd0                 20G  1.1G   19G   6% /mnt/ceph-vol1
	192.168.1.102:6789:/      13G     0   13G   0% /mnt/kernel_cephfs
### 旧的的方式挂载CephFS,将秘钥存储在一个文件中，并将这个文件作为秘钥的挂载选项
	[root@ceph-client1 ~]# echo "AQDKyZ9dQzpBEBAArba473MpELOhcGFlHomt1g==" > /etc/ceph/adminkey
	[root@ceph-client1 ~]# mount -t ceph 192.168.1.102:6789:/ /mnt/kernel_cephfs -o name=admin,seecretfile=/etc/ceph/adminkey
### 要在文件系统中挂载CephFS ，需要添加/etc/fstab文件
	[root@ceph-client1 ~]# vim /etc/fstab 
	192.168.1.102:6789:/ /mnt/kernel_cephfs ceph name=admin,secret=AQDKyZ9dQzpBEBAArba473MpELOhcGFlHomt1g==  0 2
### 卸载并重新挂载Ceph_FS
	[root@ceph-client1 ~]# umount /mnt/kernel_cephfs/	
	[root@ceph-client1 ~]# mount /mnt/kernel_cephfs/	
	[root@ceph-client1 ~]# df -h
	Filesystem               Size  Used Avail Use% Mounted on
	devtmpfs                 465M     0  465M   0% /dev
	tmpfs                    478M     0  478M   0% /dev/shm
	tmpfs                    478M  6.7M  472M   2% /run
	tmpfs                    478M     0  478M   0% /sys/fs/cgroup
	/dev/mapper/centos-root   17G  1.6G   16G  10% /
	/dev/sda1               1014M  169M  846M  17% /boot
	tmpfs                     96M     0   96M   0% /run/user/0
	/dev/rbd0                 20G  1.1G   19G   6% /mnt/ceph-vol1
	192.168.1.102:6789:/      13G     0   13G   0% /mnt/kernel_cephfs
2. 使用Ceph FUSE
## Ceph FUSE适用于内核版本较低的系统
### 安装Ceph FUSE
	[root@ceph-client1 ~]# scp 192.168.1.101:/etc/yum.repos.d/ceph.repo /etc/yum.repos.d/
	[root@ceph-client1 ~]# yum install -y ceph-fuse
### 创建挂载点目录

挂载前，确认客户端已经有了ceph配置及秘钥环文件

	[root@ceph-client1 ~]# mkdir /mnt/fuse
	
### 使用Ceph FUSE客户端挂载
	[root@ceph-client1 ~]# ceph-fuse -m 192.168.1.102:6789 /mnt/fuse
	ceph-fuse[16200]: starting ceph client
	2019-10-13 20:51:50.990 7f3182170e00 -1 init, newargv = 0x55e716b983e0 newargc=7
	ceph-fuse[16200]: starting fuse
	[root@ceph-client1 ~]# df -h
	Filesystem               Size  Used Avail Use% Mounted on
	devtmpfs                 465M     0  465M   0% /dev
	tmpfs                    478M     0  478M   0% /dev/shm
	tmpfs                    478M  6.7M  472M   2% /run
	tmpfs                    478M     0  478M   0% /sys/fs/cgroup
	/dev/mapper/centos-root   17G  2.0G   16G  12% /
	/dev/sda1               1014M  169M  846M  17% /boot
	tmpfs                     96M     0   96M   0% /run/user/0
	/dev/rbd0                 20G  1.1G   19G   6% /mnt/ceph-vol1
	192.168.1.102:6789:/      13G     0   13G   0% /mnt/kernel_cephfs
	ceph-fuse                 13G     0   13G   0% /mnt/fuse
	[root@ceph-client1 ~]# 
### 要在文件系统中挂载CephFS ，需要添加/etc/fstab文件
	[root@ceph-client1 ~]# vim /etc/fstab 
	id=admin /mnt/fuse fuse.ceph default 0 0
### 卸载并重新挂载cephfs
	[root@ceph-client1 ~]# umount /mnt/fuse/
	[root@ceph-client1 ~]# mount /mnt/fuse/
	ceph-fuse[16310]: starting ceph client
	2019-10-13 20:56:54.126 7f1d4c008e00 -1 init, newargv = 0x5575fc268320 newargc=9
	ceph-fuse[16310]: starting fuse
