# Ceph 缓存分层
> https://docs.ceph.com/docs/master/rados/operations/cache-tiering/

缓存分层是在更快的磁盘（通常是SSD），上创建一个Ceph池。这个缓存池应该放置在一个常规的复制池或erasure池的前端，这样所有的客户端I/O操作都首先由缓存池处理。之后，再将数据写回到现有的数据池中。

客户端能够在缓存池上享受高性能，而它们的数据显而易见最终写入到常规池中的。

一般来说，缓存层构建在昂贵/速度更快的SSD磁盘上，这样才能为客户提供更好的I/O性能。在缓存池后端通常是存储层，它由复制或者erasure类型的HDD组成。在这种类型的设置中，客户端将I/O请求提交到缓存池，不管它是一个读或写操作，它请求都能够立即获得响应。速度更快的缓存层为客户端提供服务。一段时间后，缓存层将所有数据写回备用的存储层，以便它客户缓存来自客户端的新请求。在缓存层和存储层之间的数据迁移都是自动触发且对客户端是透明的。缓存层能以下模式进行配置。

## writeback模式：
当Ceph的缓存分层配置为writeback模式时。Ceph客户端将数据写入缓存层类型的池，也就是速度更快的池，因此能够立即接收写入确认。基于为缓存层设置的flushing/evicting策略，数据将从缓存层迁移到存储层，并最终由缓存层代理将其从缓存层删除。处理客户端读操作时，首先由缓存分层代理将数据从存储层迁移到缓存层，然后再把它提供给客户。知道数据变得不在活跃成为冷数据，否则它将一直保留在缓存层中。
 
## readonly模式
当Ceph的缓存分层配置为read-only模式时，它只适用与处理客户端的读操作。客户端的写操作不涉及缓存分层，所有的客户端都写在存储层上完成。在处理来自客户端的读操作时，缓存分层代理将请求的数据复制到缓存层。基于为缓存层配置的策略，不活跃的对象将从缓存层中删除。这种方法非常适合多个客户端需要兑取大量类似的数据的场景。
## 部署缓存池

缓存层是在速度更快的物理磁盘（通常是SSD），上实现的，它在使用HDD构建的速度较慢的常规池前部署一个快速的缓存层。在本实验中，我们将创建两个独立的池（一个缓存池和一个常规），分别用作缓存层和存储层。
### 新建池
创建一个缓存池基于（osd.0、osd.3、osd.6、）。事实上，当前环境并没有SSD，建设这些OSD是SSD并在其上创建一个缓存池。

### 获取CRUSH map并反编译它
	[root@ceph-node1 ~]# ceph osd getcrushmap -o crushmapdump
	[root@ceph-node1 ~]# crushtool -d crushmapdump -o crushmapdump-complied 
### 编辑反编译得到的CRUSH map文件，在root default 下加入如下内容
	root cache {
	        id -10          # do not change unnecessarily
	        id -9 class hdd         # do not change unnecessarily
	        # weight 0.027
	        alg straw2
	        hash 0  # rjenkins1
	        item osd.0 weight 0.009
	        item osd.3 weight 0.009
	        item osd.6 weight 0.009
	}

**根据实际环境修改CRUSH map文件**

### 创建rule
	rule cache-pool {
	        id 4
	        type replicated
	        min_size 1
	        max_size 10
	        step take cache
	        step chooseleaf firstn 0 type osd
	        step emit
	}
### 编译新的CRUSH map并导入到Ceph集群
	[root@ceph-node1 ~]# crushtool -c crushmapdump-complied -o crushmapdump
	[root@ceph-node1 ~]# ceph osd setcrushmap -i crushmapdump-complied
### 一旦应用了新的CRUSH map，检查OSD状态查看新的OSD分布。会看到一个cache的新的root bucket
	[root@ceph-node1 ~]# ceph osd tree
	ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF 
	-10       0.02696 root cache                                  
	  0   hdd 0.00899     osd.0               up  1.00000 1.00000 
	  3   hdd 0.00899     osd.3               up  1.00000 1.00000 
	  6   hdd 0.00899     osd.6               up  1.00000 1.00000 
	 -1       0.07796 root default                                
	 -3       0.02599     host ceph-node1                         
	  0   hdd 0.00899         osd.0           up  1.00000 1.00000 
	  1   hdd 0.00899         osd.1           up  1.00000 1.00000 
	  2   hdd 0.00899         osd.2           up  1.00000 1.00000 
	 -5       0.02599     host ceph-node2                         
	  3   hdd 0.00899         osd.3           up  1.00000 1.00000 
	  4   hdd 0.00899         osd.4           up  1.00000 1.00000 
	  5   hdd 0.00899         osd.5           up  1.00000 1.00000 
	 -7       0.02599     host ceph-node3                         
	  6   hdd 0.00899         osd.6           up  1.00000 1.00000 
	  7   hdd 0.00899         osd.7           up  1.00000 1.00000 
	  8   hdd 0.00899         osd.8           up  1.00000 1.00000 
### 新建一个池并设定它的crush_ruleset是4（cache-pool）
	[root@ceph-node1 ~]# ceph osd pool create  cache-pool 32 32 cache-pool            
	pool 'cache-pool' created
	[root@ceph-node1 ~]# ceph osd dump | grep cache-pool
	pool 3 'cache-pool' replicated size 3 min_size 2 crush_rule 4 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 66 flags hashpspool stripe_width 0
### 检查cache-pool数据存放位置
#### 列出缓存池中的所有对象
	[root@ceph-node1 ~]# rados -p cache-pool ls
#### 向缓存池中添加一个临时对象，验证对象存储在正确OSD上
	[root@ceph-node1 ~]# rados -p cache-pool ls
	object
	[root@ceph-node1 ~]# ceph osd map cache-pool object
	osdmap e66 pool 'cache-pool' (3) object 'object' -> pg 3.570e3222 (3.2) -> up ([3,6,0], p3) acting ([3,6,0], p3)
#### 删除这个对象
	[root@ceph-node1 ~]# rados -p cache-pool rm object
### 新建缓存层（writeback）
#### 设置缓存池，绑定存储池作为缓存池。
	[root@ceph-node1 ~]# ceph osd tier add EC_pool cache-pool 
#### 设置缓存层模式为writeback或者readonly。本次使用writeback
	[root@ceph-node1 ~]# ceph osd tier cache-mode cache-pool writeback
	set cache-mode for pool 'cache-pool' to writeback
#### 为了把标准的所有客户端请求转到缓存池，需要设置overlay
	[root@ceph-node1 ~]# ceph osd tier set-overlay EC-pool cache-pool
	overlay for 'EC-pool' is now (or already was) 'cache-pool'
#### 检查池的细节
发现EC池的tier、read_tier和write_tier设置为3,它是缓存池的ID。

同样，对于缓存池，发现tier_of设置为2,cache_mode设置为writeback。这就设置都表明已经正确配置缓存池

	[root@ceph-node1 ~]# ceph osd dump | egrep -i "EC-pool|cache-pool"
	pool 2 'EC-pool' erasure size 5 min_size 4 crush_rule 1 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode warn last_change 69 lfor 69/69/69 flags hashpspool tiers 3 read_tier 3 write_tier 3 stripe_width 12288
	pool 3 'cache-pool' replicated size 3 min_size 2 crush_rule 4 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 69 lfor 69/69/69 flags hashpspool,incomplete_clones tier_of 2 cache_mode writeback stripe_width 0
### 配置缓存层
缓存层有几个配置选项，应该为缓存池配置一些参数以实现策略的设置。

#### 为缓存池启用hit set tracking生产级别的缓存层使用bloom过滤器（高速缓存层级）

	[root@ceph-node1 ~]# ceph osd pool set cache-pool hit_set_type bloom
	set pool 3 hit_set_type to bloom
#### 启用hit_set_count,即缓存池中的hit_set次数
	[root@ceph-node1 ~]# ceph osd pool set cache-pool hit_set_count 1
	set pool 3 hit_set_count to 1
#### 启用hit_set_period,这个是hit set 在缓存池中的有效期，以秒为单位
	[root@ceph-node1 ~]# ceph osd pool set cache-pool hit_set_period 300
	set pool 3 hit_set_period to 300
#### 设置缓存分层开始从缓存池将对象写回后端存储或删除前，允许存放的最大字节数
	[root@ceph-node1 ~]# ceph osd pool set cache-pool target_max_bytes 10000000 
#### 启用target_max_objects,它是缓存分层代理开始从缓存池中将对象写回后端存储或者删除前，允许存放的最大对象数
	[root@ceph-node1 ~]# ceph osd pool set cache-pool target_max_objects 10000
#### 启用cache_min_flush_age和cache_min_evict_age,他们是缓存代理将数据从缓存层刷新到存储层然后删除这些数据的最小时间间隔（以秒为单位）
	[root@ceph-node1 ~]# ceph osd pool set cache-pool cache_min_flush_age 300
	[root@ceph-node1 ~]# ceph osd pool set cache-pool cache_min_evict_age 300  
#### 启用cache_target_dirty_ratio，它是缓存分层代理开始将数据写回存储前，允许缓存池中被修改的数据总量所占池总量的百分比
	[root@ceph-node1 ~]# ceph osd pool set cache-pool cache_target_dirty_ratio 0.4
#### 启用cache_target_full_ratio,它是缓存分层代理开始将数据写回存储层前，允许缓存池存放未经修改的数据总量站缓存池总容量的百分比
	[root@ceph-node1 ~]# ceph osd pool set cache-pool cahce_target_full_tatio 0.8
#### 新建一个500MB的临时文件，并将它放入EC-pool池中，最终它会被写入一个缓存池中
	[root@ceph-node1 ~]# dd if=/dev/zero of=/tmp/file1 bs=1M count=500
#### 因为EC-pool已经和一个缓存池绑定，所以file1不会在第一时间就写入EC-pool。应该先写入缓存池。列出没个池的对象名。使用date命令来跟踪变化
	[root@ceph-node1 ~]# rados -p EC-pool put object1 /tmp/file1
	[root@ceph-node1 ~]# rados -p EC-pool ls
	[root@ceph-node1 ~]# rados -p cache-pool ls
	[root@ceph-node1 ~]# date
#### 超出cache_min_evict_age设置的时间后，缓存池将数据写入EC-pool
	[root@ceph-node1 ~]# date
	[root@ceph-node1 ~]# rados -p EC-pool ls
	[root@ceph-node1 ~]# rados -p cache-pool ls
	如果缓存池仍然有对象，您可以手动刷新它们。 例如：
	rados -p {cachepool} cache-flush-evict-all

注意Ceph不能自动确定缓存池的大小，所以这里需要绝对大小的配置，否则flush / evict将不起作用。 如果同时指定了这两个限制，则当触发任何阈值时，缓存分层代理将开始刷新或逐出。

注意仅当达到target_max_bytes或target_max_objects时，所有客户端请求才会被阻止
如果缓存池仍然有对象，您可以手动刷新它们。 例如：
rados -p {cachepool} cache-flush-evict-all


# 使用RADOS bash对Ceph进行基准测试

该工具的语法为：rados bench -p <pool_name> <seconds> <write|seq|rand> -b <block size> -t --no-cleanup

pool_name：测试所针对的存储池
seconds：测试所持续的秒数
<write|seq|rand>：操作模式，write：写，seq：顺序读；rand：随机读
-b：block size，即块大小，默认为 4M
-t：读/写并行数，默认为 16
--no-cleanup 表示测试完成后不删除测试用数据。在做读测试之前，需要使用该参数来运行一遍写测试来产生测试数据，在全部测试结束后可以运行 rados -p <pool_name> cleanup 来清理所有测试数据。


## 写测试
	rados bench -p EC-pool 10 write --no-cleanup

## 顺序读测试
	rados bench -p EC-pool 10 seq
## 随机读测试
	rados bench -p EC-pool 10 rand
	

